{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOMS Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ArgCheck\n",
    "using Distributions\n",
    "using HMMBase\n",
    "using ParsimoniousMonitoring\n",
    "using ProgressMeter\n",
    "using PyPlot\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using POMDPSimulators\n",
    "using DiscreteValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement only one route in receding horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Synthetic scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRE(reference, candidate) = mean((candidate .- reference) ./ reference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(states(smdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Three paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = HMM([0.9 0.1; 0.1 0.9], [Constant(1.0), Constant(3.0)])\n",
    "p2 = HMM([0.8 0.2; 0.3 0.7], [Constant(0.5), Constant(4.0)])\n",
    "p3 = HMM([0.65 0.35; 0.35 0.65], [Constant(0.25), Constant(3.75)])\n",
    "mdp = MonitoringMDP([p1, p2, p3], [20, 10, 10], [0.5, 0.5, 0.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(states(mdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RecedingHorizonPolicy(mdp, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@showprogress for state in states(mdp)\n",
    "    action(policy, state)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 A first simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A discrete probability distribution with a single value.\n",
    "constdist(x) = DiscreteNonParametric([x], [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic path\n",
    "p1 = HMM(ones(1,1), [constdist(8.0)])\n",
    "# Stochatich path\n",
    "p2 = HMM([0.99 0.01; 0.02 0.98], [constdist(5.0), constdist(10.0)])\n",
    "# τmax = 150, c = 0.65\n",
    "mdp = MonitoringMDP([p1, p2], [150, 150], [0, 0.65])\n",
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hcat(rand(mdp.models[1], 3000), rand(mdp.models[2], 3000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(data[:,1], label = \"Deterministic path\")\n",
    "ax.plot(data[:,2], label = \"Stochastic path\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"$L(t)$\", ylim = (4, 12))\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one stochastic path with two states, we can compute the greedy threshold policy analytically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function thresholds(mdp::MonitoringMDP{2})\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    l0, l1 = mean.(mdp.models[2].B)\n",
    "    c / (l - l0), 1 - c / (l1 - l)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = thresholds(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we benchmark against a generic MDP greedy policy, and we verify that it matches the analytical thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logbook_greedy = benchmark(mdp, GreedyPolicy(mdp), data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instants = findall(map(h -> h.a[2], logbook_greedy));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(data[:,1], label = \"Deterministic path\")\n",
    "ax.plot(data[:,2], label = \"Stochastic path\")\n",
    "ax.scatter(instants, data[instants,2], c = \"red\", marker = \"o\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"$L(t)$\", ylim = (4, 12))\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = map(logbook_greedy) do history\n",
    "    state = history.s[2]\n",
    "    (mdp.models[2].A^(state.timesteps+1))[state.laststate,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(predictor)\n",
    "ax.axhline(xmin, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmin\")\n",
    "ax.axhline(xmax, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmax\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"γ_{t-1,t}(1)\", ylim = (0, 1.0))\n",
    "ax.legend(loc = \"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# # In this case the belief space is a line [0,1] which represents \n",
    "# # the probability of the stochastic path being in state 1.\n",
    "# policy = GreedyPolicy(mdp)\n",
    "# greedy_actions = map(states(mdp)) do state\n",
    "#     action(policy, state), (mdp.models[2].A^state[2].timesteps)[state[2].laststate,1]\n",
    "# end\n",
    "\n",
    "# # policy = map(states(mdp))\n",
    "\n",
    "# # # Order the policy by belief values, and find the thresholds\n",
    "# # perm = sortperm(belief_1d)\n",
    "# # sorted_belief, sorted_policy = belief_1d[perm], policy.policy[perm]\n",
    "# # sorted_belief[findall(sorted_policy[2:end] .!= sorted_policy[1:end-1]) .+ 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=5000, belres=1e-6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mdp_99 = solve_sparse(solver, mdp, smdp, 0.99);\n",
    "logbook_mdp_99 = benchmark(mdp, policy_mdp_99, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logbook_never = benchmark(mdp, never_measure_policy(2), data)\n",
    "logbook_always = benchmark(mdp, always_measure_policy(2), data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{G} = 1_{C(t)=1}(l - L(t)) - c 1_{M(t)=1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gain(mdp::MonitoringMDP, logbook)\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    map(logbook) do history\n",
    "        ((history.path == 2) * (l - history.delay))\n",
    "    end\n",
    "end\n",
    "\n",
    "function penalized_gain(mdp::MonitoringMDP, logbook)\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    map(logbook) do history\n",
    "        ((history.path == 2) * (l - history.delay)) - (c * history.a[2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we gain something when we never measure?  \n",
    "=> Because on average the stochastic path is shorter: 7.5ms vs 8ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_never)), label = \"Never measure\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_always)), label = \"Always measure\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_greedy)), label = \"Greedy policy\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_mdp_99)), label = \"MDP 0.99\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = \"Cumulative penalized gain\")\n",
    "ax.legend(loc = \"upper right\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(cumsum(gain(mdp, logbook_never)), label = \"Never measure\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_always)), label = \"Always measure\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_greedy)), label = \"Greedy policy\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_mdp_99)), label = \"MDP 0.99\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = \"Cumulative gain\")\n",
    "ax.legend(loc = \"upper right\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mdp_01 = solve_sparse(solver, mdp, smdp, 0.01);\n",
    "policy_mdp_50 = solve_sparse(solver, mdp, smdp, 0.50);\n",
    "policy_mdp_99 = solve_sparse(solver, mdp, smdp, 0.99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = Dict(\n",
    "    \"Never measure\"  => never_measure_policy(2),\n",
    "    \"Always measure\" => always_measure_policy(2),\n",
    "    \"Greedy policy\"  => GreedyPolicy(mdp),\n",
    "    \"MDP 0.01\" => policy_mdp_01,\n",
    "    \"MDP 0.50\" => policy_mdp_50,\n",
    "    \"MDP 0.99\" => policy_mdp_99,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_average(logbooks)\n",
    "    Dict(\n",
    "        \"Average Measures\"  => mean(logbook -> sum(h -> h.a[2], logbook), logbooks),\n",
    "        \"Average Penalized Gain\" => mean(logbook -> mean(penalized_gain(mdp, logbook)), logbooks)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mc(mdp, policies, 100, 3000, summary_fn = simple_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief_1d, ones(length(belief_1d)), c = policy.policy, s = 1.0)\n",
    "ax.axvline.([xmin, xmax], lw = 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOMS paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dire dans la these qu'on peut appliquer RH en ligne puisque pas besoin de visiter tout les états."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ParsimoniousMonitoring: OnlineRecedingHorizonPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps = 3000, show_progress = true)\n",
    "s0 = rand(states(mdp))\n",
    "h_always = simulate(hr, mdp, ConstantPolicy((false,true)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy((false,false)), s0);\n",
    "h_mdp = simulate(hr, mdp, policy, s0);\n",
    "h_rh = simulate(hr, mdp,  OnlineRecedingHorizonPolicy(mdp, 4), s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(4,4))\n",
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_rh.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function belief_1d(mdp::MonitoringMDP, p::Int, k::Int)\n",
    "#     states_ = states(mdp)\n",
    "#     belief = Vector{Float64}(undef, length(states_))\n",
    "#     model = mdp.models[p]\n",
    "#     for (i, state) in enumerate(states_)\n",
    "#         belief[i] = (model.A^state[p].timesteps)[state[p].laststate,k]\n",
    "#     end\n",
    "#     belief\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief = belief_1d(mdp, 2, 1)\n",
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief, ones(length(belief)), c = policy.policy, s = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps=3000)\n",
    "s0 = rand(states(mdp))\n",
    "h_always = simulate(hr, mdp, ConstantPolicy((false,true)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy((false,false)), s0);\n",
    "h_mdp = simulate(hr, mdp, policy, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(states(mdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function belief_1d(mdp::MonitoringMDP, p::Int, k::Int)\n",
    "    states_ = states(mdp)\n",
    "    belief = Vector{Float64}(undef, length(states_))\n",
    "    model = mdp.models[p]\n",
    "    for (i, (state)) in enumerate(states_)\n",
    "        timesteps, laststate = getstate(state)[p]\n",
    "        belief[i] = (model.A^timesteps)[laststate,k]\n",
    "    end\n",
    "    belief\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief = belief_1d(mdp, 2, 1)\n",
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief, ones(length(belief)), c = res.policy, s = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ConstantPolicy <: Policy\n",
    "    action::CartesianIndex\n",
    "end\n",
    "POMDPs.action(policy::ConstantPolicy, _) = policy.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDPPolicy <: Policy\n",
    "    mdp::MonitoringMDP\n",
    "    policy::Vector{Int}\n",
    "end\n",
    "\n",
    "function MDPPolicy(mdp::MonitoringMDP, policy::ValueIterationPolicy)\n",
    "    MDPPolicy(mdp, policy.policy)\n",
    "end\n",
    "\n",
    "function POMDPs.action(policy::MDPPolicy, s)\n",
    "    state = stateindex(mdp, s)\n",
    "    action = policy.policy[state]\n",
    "    actions(mdp)[action]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pol = ConstantPolicy(CartesianIndex(1,1))\n",
    "# pol = MDPPolicy(mdp, res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = RolloutSimulator(max_steps=10)\n",
    "# r = simulate(rs, mdp, pol, rand(mdp.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = rand(mdp.states);\n",
    "# s0 = CartesianIndex(0, 1, 0, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps=3000)\n",
    "h_always = simulate(hr, mdp, ConstantPolicy(CartesianIndex(0,1)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy(CartesianIndex(0,0)), s0);\n",
    "h_mdp = simulate(hr, mdp, MDPPolicy(mdp, res), s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(map(x -> x[:a] == CartesianIndex(0,1), h_mdp.hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(map(x -> x[:r], h_mdp.hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Defining-a-Heuristic-Policy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_greedy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    action = getaction(actions(mdp)[action])\n",
    "    timesteps, laststate = getstate(states(mdp)[i])[2]\n",
    "    push!(x, (p2.A^timesteps)[laststate,1])\n",
    "    push!(y, action[2])\n",
    "end\n",
    "scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    action = getaction(actions(mdp)[action])\n",
    "    timesteps, laststate = getstate(states(mdp)[i])[2]\n",
    "    push!(x, (p2.A^timesteps)[laststate,1])\n",
    "    push!(y, action[2])\n",
    "end\n",
    "scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Two Markov chains of two states each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use DiscreteNonParametric instead of 0-variance Normal distn.\n",
    "p1 = HMM([0.7 0.3; 0.3 0.7], [Normal(0.5, 0), Normal(2.0, 0)])\n",
    "p2 = HMM([0.9 0.1; 0.1 0.9], [Normal(1.0,0), Normal(3.0,0)])\n",
    "mdp = MonitoringMDP([100, 100], [p1, p2], [0.05, 0.15], 0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=100, belres=1e-6, verbose=true)\n",
    "res = solve(solver, smdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = [], [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    state = getstate(states(mdp)[i])\n",
    "    timesteps, laststate = state[1]\n",
    "    push!(x, (p1.A^(timesteps+1))[laststate,1])\n",
    "    timesteps, laststate = state[2]\n",
    "    push!(y, (p2.A^(timesteps+1))[laststate,1])\n",
    "    push!(z, action)\n",
    "end\n",
    "# scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x, y, c=z)\n",
    "xlim(0,1)\n",
    "ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement https://juliapomdp.github.io/POMDPModelTools.jl/latest/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://juliapomdp.github.io/POMDPSimulators.jl/stable/parallel/#Parallel-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
