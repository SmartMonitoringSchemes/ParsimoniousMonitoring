{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JONS Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ArgCheck\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using HMMBase\n",
    "using ParsimoniousMonitoring\n",
    "using PyPlot\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using POMDPSimulators\n",
    "using DiscreteValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement only one route in receding horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 A first simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A discrete probability distribution with a single value.\n",
    "constdist(x) = DiscreteNonParametric([x], [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic path\n",
    "p1 = HMM(ones(1,1), [constdist(8.0)])\n",
    "# Stochatich path\n",
    "p2 = HMM([0.99 0.01; 0.02 0.98], [constdist(5.0), constdist(10.0)])\n",
    "# τmax = 150, c = 0.65\n",
    "mdp = MonitoringMDP([p1, p2], [150, 150], [0, 0.65])\n",
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hcat(rand(mdp.models[1], 3000), rand(mdp.models[2], 3000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(data[:,1], label = \"Deterministic path\")\n",
    "ax.plot(data[:,2], label = \"Stochastic path\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"$L(t)$\", ylim = (4, 12))\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one stochastic path with two states, we can compute the greedy threshold policy analytically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function thresholds(mdp::MonitoringMDP{2})\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    l0, l1 = mean.(mdp.models[2].B)\n",
    "    c / (l - l0), 1 - c / (l1 - l)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = thresholds(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we benchmark against a generic MDP greedy policy, and we verify that it matches the analytical thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logbook_greedy = benchmark(mdp, GreedyPolicy(mdp), data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instants = findall(map(h -> h.a[2], logbook_greedy));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(data[:,1], label = \"Deterministic path\")\n",
    "ax.plot(data[:,2], label = \"Stochastic path\")\n",
    "ax.scatter(instants, data[instants,2], c = \"red\", marker = \"o\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"$L(t)$\", ylim = (4, 12))\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = map(logbook_greedy) do history\n",
    "    state = history.s[2]\n",
    "    (mdp.models[2].A^(state.timesteps+1))[state.laststate,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(predictor)\n",
    "ax.axhline(xmin, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmin\")\n",
    "ax.axhline(xmax, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmax\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = L\"γ_{t-1,t}(1)\", ylim = (0, 1.0))\n",
    "ax.legend(loc = \"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# # In this case the belief space is a line [0,1] which represents \n",
    "# # the probability of the stochastic path being in state 1.\n",
    "# policy = GreedyPolicy(mdp)\n",
    "# greedy_actions = map(states(mdp)) do state\n",
    "#     action(policy, state), (mdp.models[2].A^state[2].timesteps)[state[2].laststate,1]\n",
    "# end\n",
    "\n",
    "# # policy = map(states(mdp))\n",
    "\n",
    "# # # Order the policy by belief values, and find the thresholds\n",
    "# # perm = sortperm(belief_1d)\n",
    "# # sorted_belief, sorted_policy = belief_1d[perm], policy.policy[perm]\n",
    "# # sorted_belief[findall(sorted_policy[2:end] .!= sorted_policy[1:end-1]) .+ 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=5000, belres=1e-6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mdp_99 = solve_sparse(solver, mdp, smdp, 0.99);\n",
    "logbook_mdp_99 = benchmark(mdp, policy_mdp_99, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logbook_never = benchmark(mdp, never_measure_policy(2), data)\n",
    "logbook_always = benchmark(mdp, always_measure_policy(2), data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{G} = 1_{C(t)=1}(l - L(t)) - c 1_{M(t)=1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gain(mdp::MonitoringMDP, logbook)\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    map(logbook) do history\n",
    "        ((history.path == 2) * (l - history.delay))\n",
    "    end\n",
    "end\n",
    "\n",
    "function penalized_gain(mdp::MonitoringMDP, logbook)\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    map(logbook) do history\n",
    "        ((history.path == 2) * (l - history.delay)) - (c * history.a[2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we gain something when we never measure?  \n",
    "=> Because on average the stochastic path is shorter: 7.5ms vs 8ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_never)), label = \"Never measure\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_always)), label = \"Always measure\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_greedy)), label = \"Greedy policy\")\n",
    "ax.plot(cumsum(penalized_gain(mdp, logbook_mdp_99)), label = \"MDP 0.99\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = \"Cumulative penalized gain\")\n",
    "ax.legend(loc = \"upper right\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(cumsum(gain(mdp, logbook_never)), label = \"Never measure\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_always)), label = \"Always measure\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_greedy)), label = \"Greedy policy\")\n",
    "ax.plot(cumsum(gain(mdp, logbook_mdp_99)), label = \"MDP 0.99\")\n",
    "ax.set(xlabel = \"Timestep\", ylabel = \"Cumulative gain\")\n",
    "ax.legend(loc = \"upper right\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mdp_01 = solve_sparse(solver, mdp, smdp, 0.01);\n",
    "policy_mdp_50 = solve_sparse(solver, mdp, smdp, 0.50);\n",
    "policy_mdp_99 = solve_sparse(solver, mdp, smdp, 0.99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = Dict(\n",
    "    \"Never measure\"  => never_measure_policy(2),\n",
    "    \"Always measure\" => always_measure_policy(2),\n",
    "    \"Greedy policy\"  => GreedyPolicy(mdp),\n",
    "    \"MDP 0.01\" => policy_mdp_01,\n",
    "    \"MDP 0.50\" => policy_mdp_50,\n",
    "    \"MDP 0.99\" => policy_mdp_99,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_average(logbooks)\n",
    "    Dict(\n",
    "        \"Average Measures\"  => mean(logbook -> sum(h -> h.a[2], logbook), logbooks),\n",
    "        \"Average Penalized Gain\" => mean(logbook -> mean(penalized_gain(mdp, logbook)), logbooks)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = benchmark_mc(mdp, policies, 100, 3000, summary_fn = simple_average);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(unstack(stack(df), :policy, :value), allcols = true, splitcols = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analytical number of measurements (see end of section 8.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Two Markov chains of two states each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use DiscreteNonParametric instead of 0-variance Normal distn.\n",
    "p1 = HMM([0.7 0.3; 0.3 0.7], [constdist(0.5), constdist(2.0)])\n",
    "p2 = HMM([0.9 0.1; 0.1 0.9], [constdist(1.0), constdist(3.0)])\n",
    "mdp = MonitoringMDP([p1, p2], [150, 150], [0.05, 0.15]);\n",
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Greedy policy on a continuous grid (Table 3 JONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=5000, belres=1e-6)\n",
    "policy_mdp_01 = solve_sparse(solver, mdp, smdp, 0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_2d = zeros(length(states(mdp)), 2)\n",
    "for (i, state) in enumerate(states(mdp))\n",
    "    predictor_p1 = ContinuousBelief(predict(state[1]), mdp.models[1])\n",
    "    predictor_p2 = ContinuousBelief(predict(state[2]), mdp.models[2])\n",
    "    belief_2d[i,1] = predictor_p1.belief[1]\n",
    "    belief_2d[i,2] = predictor_p2.belief[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots()\n",
    "ax.scatter(belief_2d[:,1], belief_2d[:,2], c = policy_mdp_01.policy)\n",
    "ax.set(xlim = [0, 1], ylim = [0, 1])\n",
    "# ax.legend() # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
