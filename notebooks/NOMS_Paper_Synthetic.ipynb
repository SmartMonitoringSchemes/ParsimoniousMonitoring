{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOMS Paper Synthetic (5.1)\n",
    "**TODO: Move evaluation code to a script and merge this notebook with NOMS_Paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using HMMBase\n",
    "using JSON\n",
    "using ParsimoniousMonitoring\n",
    "using ProgressMeter\n",
    "using PyPlot\n",
    "using ThesisTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using DiscreteValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRE(baseline, candidate) = mean((candidate .- baseline) ./ baseline);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function eval_policies(mdp, policies)\n",
    "    results = Dict{String,Float64}()\n",
    "\n",
    "    # Baseline VI policy\n",
    "    solver = SparseValueIterationSolver(max_iterations = 5000)\n",
    "\n",
    "    timing = @timed baseline_policy = solve(solver, mdp)\n",
    "    baseline = evaluate(mdp, baseline_policy).(states(mdp))\n",
    "    println(\"Baseline: $(timing[2])s\")\n",
    "\n",
    "    # Candidate policies\n",
    "    for (name, policy_fn) in policies\n",
    "        policy = CachedPolicy(mdp, policy_fn(mdp))\n",
    "        timing = @timed vf = evaluate(mdp, policy).(states(mdp))\n",
    "        println(\"$(name): $(timing[2])s\")\n",
    "        results[name] = MRE(baseline, vf)\n",
    "    end\n",
    "\n",
    "    results\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function synthetic_mdp(β, τmax)\n",
    "    p1 = p2 = HMM([β 1 - β; 1 - β β], [Constant(0.01), Constant(100)])\n",
    "    MonitoringMDP([p1, p2], [τmax, τmax], [25, 25], 0.99)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = Dict{String,Function}(\n",
    "    # \"Greedy\" => mdp -> GreedyPolicy(mdp),\n",
    "    \"Greedy\" => mdp -> AnalyticalGreedyPolicy(mdp),\n",
    "    \"RH-2\" => mdp -> RecedingHorizonPolicy(mdp, 2, shared_cache = true),\n",
    "    \"RH-3\" => mdp -> RecedingHorizonPolicy(mdp, 3, shared_cache = true),\n",
    "    \"RH-4\" => mdp -> RecedingHorizonPolicy(mdp, 4, shared_cache = true),\n",
    "    \"Heuristic\" => mdp -> HeuristicPolicy(mdp, SparseValueIterationSolver(max_iterations = 100, include_Q = false))\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# βs = vcat(0.01:0.01:0.05, 0.1:0.1:0.9, 0.95:0.01:0.99)\n",
    "# results = Vector{Dict}(undef, size(βs))\n",
    "\n",
    "# for i in eachindex(βs)\n",
    "#     mdp = synthetic_mdp(βs[i], 150)\n",
    "#     results[i] = eval_policies(mdp, policies)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = DataFrame([Float64[], String[], Float64[]], [:β, :policy, :mre])\n",
    "# for (β, result) in zip(βs, results)\n",
    "#     for (policy, mre) in result\n",
    "#         push!(df, (β, policy, mre))\n",
    "#     end\n",
    "# end\n",
    "# df = unstack(df, :policy, :mre)\n",
    "# CSV.write(\"../results/noms_synthetic.csv\", df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(\"../results/noms_synthetic.csv\")\n",
    "first(df, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (7,5))\n",
    "for (policy, label) in Dict(\"Greedy\" => \"Myope\", \"Heuristic\" => \"Heuristique\", \"RH-2\" => \"RH-2\", \"RH-3\" => \"RH-3\", \"RH-4\" => \"RH-4\")\n",
    "    ax.plot(df[:,:β], df[:,Symbol(policy)] * 100, label = label)\n",
    "end\n",
    "ax.grid()\n",
    "ax.legend(loc = \"upper left\", ncol = 5)\n",
    "ax.set(xticks = 0:0.1:1, yticks = 0:10:70, xlabel = L\"$\\beta$\", ylabel = \"MRE (%)\")\n",
    "save_thesis(\"heuristics_mre\", clean = true, hwr = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function synthetic_mdp_3p(c, ρ)\n",
    "    p1 = HMM([0.9 0.1; 0.1 0.9], [Constant(1), Constant(3)])\n",
    "    p2 = HMM([0.8 0.2; 0.3 0.7], [Constant(0.5), Constant(4)])\n",
    "    p3 = HMM([0.65 0.35; 0.35 0.65], [Constant(0.25), Constant(3.75)])\n",
    "    mdp = MonitoringMDP([p1, p2, p3], [20, 20, 20], fill(c, 3), ρ)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = Dict{String,Function}(\n",
    "    \"Greedy\" => mdp -> GreedyPolicy(mdp),\n",
    "    \"RH-3\" => mdp -> RecedingHorizonPolicy(mdp, 3, shared_cache = true),\n",
    "    \"Heuristic\" => mdp -> HeuristicPolicy(mdp, SparseValueIterationSolver(max_iterations = 100, include_Q = false))\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs = [0.0625, 0.125, 0.25, 0.5]\n",
    "# ρs = [0.99, 0.999]\n",
    "# results = Matrix{Dict}(undef, length(cs), length(ρs))\n",
    "\n",
    "# for i in eachindex(cs), j in eachindex(ρs)\n",
    "#     mdp = synthetic_mdp_3p(cs[i], ρs[j])\n",
    "#     results[i,j] = eval_policies(mdp, policies)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = DataFrame([Float64[], Float64[], String[], Float64[]], [:c, :ρ, :policy, :mre])\n",
    "# for (i, c) in enumerate(cs), (j, ρ) in enumerate(ρs)\n",
    "#     for (policy, mre) in results[i,j]\n",
    "#         push!(df, (c, ρ, policy, mre * 100))\n",
    "#     end\n",
    "# end\n",
    "# df = unstack(df, :policy, :mre)\n",
    "# CSV.write(\"../results/noms_synthetic_3p.csv\", df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(\"../results/noms_synthetic_3p.csv\")\n",
    "sort(df, (:ρ, order(:c, rev=true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
