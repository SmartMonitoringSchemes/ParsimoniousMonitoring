{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JONS Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ArgCheck\n",
    "using Distributions\n",
    "using HMMBase\n",
    "using ParsimoniousMonitoring\n",
    "using PyPlot\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using POMDPSimulators\n",
    "using DiscreteValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement only one route in receding horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 A first simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A discrete probability distribution with a single value.\n",
    "constdist(x) = DiscreteNonParametric([x], [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic path\n",
    "p1 = HMM(ones(1,1), [constdist(8.0)])\n",
    "# Stochatich path\n",
    "p2 = HMM([0.99 0.01; 0.02 0.98], [constdist(5.0), constdist(10.0)]);\n",
    "# τmax = 150, c = 0.65\n",
    "mdp = MonitoringMDP([p1, p2], [150, 150], [0, 0.65]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(rand(p1, 3000), label=\"Deterministic path\")\n",
    "ax.plot(rand(p2, 3000), label=\"Stochastic path\")\n",
    "ax.set_xlabel(\"Timestep\")\n",
    "ax.set_ylabel(L\"$L(t)$\")\n",
    "ax.set_ylim(4, 12)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function thresholds(mdp::MonitoringMDP{2})\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    l0, l1 = mean.(mdp.models[2].B)\n",
    "    c / (l - l0), 1 - c / (l1 - l)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = thresholds(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr = HistoryRecorder(max_steps = 3000, show_progress = true)\n",
    "# s0 = rand(states(mdp))\n",
    "# h_greedy = simulate(hr, mdp, GreedyPolicy(mdp), s0)\n",
    "# # h_always = simulate(hr, mdp, always_measure_policy(P), s0);\n",
    "# # h_never = simulate(hr, mdp, ConstantPolicy((false,false)), s0);\n",
    "# # h_mdp = simulate(hr, mdp, policy, s0);\n",
    "# # h_rh = simulate(hr, mdp,  OnlineRecedingHorizonPolicy(mdp, 4), s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = map(h_greedy) do history\n",
    "#     state = history.s[2]\n",
    "#     (mdp.models[2].A^(state.timesteps+1))[state.laststate,1]\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = subplots(figsize = (10, 3))\n",
    "# ax.plot(predictor)\n",
    "# ax.axhline(xmin, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmin\")\n",
    "# ax.axhline(xmax, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmax\")\n",
    "# ax.set_xlabel(\"Timestep\")\n",
    "# ax.set_ylabel(L\"γ_{t-1,t}(1)\")\n",
    "# ax.set_ylim(0, 1.0)\n",
    "# ax.legend(loc = \"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logbook = benchmark(mdp, GreedyPolicy(mdp), hcat(rand(mdp.models[1], 3000), rand(mdp.models[2], 3000)));\n",
    "logbook = benchmark(mdp, RecedingHorizonPolicy(mdp, 4), hcat(rand(mdp.models[1], 3000), rand(mdp.models[2], 3000)), show_progress = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = map(logbook) do history\n",
    "    state = history.s[2]\n",
    "    (mdp.models[2].A^(state.timesteps+1))[state.laststate,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?@timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(predictor)\n",
    "ax.axhline(xmin, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmin\")\n",
    "ax.axhline(xmax, c = \"black\", ls = \"--\", lw = 1.0, label = \"xmax\")\n",
    "ax.set_xlabel(\"Timestep\")s\n",
    "ax.set_ylabel(L\"γ_{t-1,t}(1)\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(loc = \"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "fig, ax = subplots(figsize = (10, 3))\n",
    "ax.plot(DataFrame(logbook).delay)\n",
    "ax.plot(DataFrame(logbook).delay_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logbook = benchmark(mdp, GreedyPolicy(mdp), hcat(rand(mdp.models[1], 3000), rand(mdp.models[2], 3000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_greedy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1 = HMM(ones(1,1), [Normal(8,0)])\n",
    "p2 = HMM([0.99 0.01; 0.02 0.98], [Normal(5,0), Normal(10,0)])\n",
    "mdp = MonitoringMDP([p1, p2], [150, 150], [0.65, 0.65]);\n",
    "@time smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=1000, belres=1e-6)\n",
    "policy = solve_sparse(solver, mdp, smdp, 0.99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case the belief space is a line [0,1] which represents \n",
    "# the probability of the stochastic path being in state 1.\n",
    "belief_1d = map(states(mdp)) do state\n",
    "    (mdp.models[2].A^state[2].timesteps)[state[2].laststate,1]\n",
    "end;\n",
    "\n",
    "# Order the policy by belief values, and find the thresholds\n",
    "perm = sortperm(belief_1d)\n",
    "sorted_belief, sorted_policy = belief_1d[perm], policy.policy[perm]\n",
    "sorted_belief[findall(sorted_policy[2:end] .!= sorted_policy[1:end-1]) .+ 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function thresholds(mdp::MonitoringMDP{2})\n",
    "    @argcheck size(mdp.models[1], 1) == 1 # Deterministic link\n",
    "    @argcheck size(mdp.models[2], 1) == 2 # Stochastic link\n",
    "    c = mdp.costs[2]\n",
    "    l = mean(mdp.models[1].B[1])\n",
    "    l0, l1 = mean.(mdp.models[2].B)\n",
    "    c / (l - l0), 1 - c / (l1 - l)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = thresholds(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief_1d, ones(length(belief_1d)), c = policy.policy, s = 1.0)\n",
    "ax.axvline.([xmin, xmax], lw = 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOMS paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dire dans la these qu'on peut appliquer RH en ligne puisque pas besoin de visiter tout les états."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ParsimoniousMonitoring: OnlineRecedingHorizonPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps = 3000, show_progress = true)\n",
    "s0 = rand(states(mdp))\n",
    "h_always = simulate(hr, mdp, ConstantPolicy((false,true)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy((false,false)), s0);\n",
    "h_mdp = simulate(hr, mdp, policy, s0);\n",
    "h_rh = simulate(hr, mdp,  OnlineRecedingHorizonPolicy(mdp, 4), s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(4,4))\n",
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_rh.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function belief_1d(mdp::MonitoringMDP, p::Int, k::Int)\n",
    "#     states_ = states(mdp)\n",
    "#     belief = Vector{Float64}(undef, length(states_))\n",
    "#     model = mdp.models[p]\n",
    "#     for (i, state) in enumerate(states_)\n",
    "#         belief[i] = (model.A^state[p].timesteps)[state[p].laststate,k]\n",
    "#     end\n",
    "#     belief\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief = belief_1d(mdp, 2, 1)\n",
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief, ones(length(belief)), c = policy.policy, s = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps=3000)\n",
    "s0 = rand(states(mdp))\n",
    "h_always = simulate(hr, mdp, ConstantPolicy((false,true)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy((false,false)), s0);\n",
    "h_mdp = simulate(hr, mdp, policy, s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(states(mdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function belief_1d(mdp::MonitoringMDP, p::Int, k::Int)\n",
    "    states_ = states(mdp)\n",
    "    belief = Vector{Float64}(undef, length(states_))\n",
    "    model = mdp.models[p]\n",
    "    for (i, (state)) in enumerate(states_)\n",
    "        timesteps, laststate = getstate(state)[p]\n",
    "        belief[i] = (model.A^timesteps)[laststate,k]\n",
    "    end\n",
    "    belief\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belief = belief_1d(mdp, 2, 1)\n",
    "fig, ax = subplots(figsize = (3, 1.0))\n",
    "ax.scatter(belief, ones(length(belief)), c = res.policy, s = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ConstantPolicy <: Policy\n",
    "    action::CartesianIndex\n",
    "end\n",
    "POMDPs.action(policy::ConstantPolicy, _) = policy.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MDPPolicy <: Policy\n",
    "    mdp::MonitoringMDP\n",
    "    policy::Vector{Int}\n",
    "end\n",
    "\n",
    "function MDPPolicy(mdp::MonitoringMDP, policy::ValueIterationPolicy)\n",
    "    MDPPolicy(mdp, policy.policy)\n",
    "end\n",
    "\n",
    "function POMDPs.action(policy::MDPPolicy, s)\n",
    "    state = stateindex(mdp, s)\n",
    "    action = policy.policy[state]\n",
    "    actions(mdp)[action]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pol = ConstantPolicy(CartesianIndex(1,1))\n",
    "# pol = MDPPolicy(mdp, res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = RolloutSimulator(max_steps=10)\n",
    "# r = simulate(rs, mdp, pol, rand(mdp.states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = rand(mdp.states);\n",
    "# s0 = CartesianIndex(0, 1, 0, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HistoryRecorder(max_steps=3000)\n",
    "h_always = simulate(hr, mdp, ConstantPolicy(CartesianIndex(0,1)), s0);\n",
    "h_never = simulate(hr, mdp, ConstantPolicy(CartesianIndex(0,0)), s0);\n",
    "h_mdp = simulate(hr, mdp, MDPPolicy(mdp, res), s0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(map(x -> x[:a] == CartesianIndex(0,1), h_mdp.hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(map(x -> x[:r], h_mdp.hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cumsum(map(x -> x[:r], h_always.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_never.hist)))\n",
    "plot(cumsum(map(x -> x[:r], h_mdp.hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JuliaPOMDP/POMDPExamples.jl/blob/master/notebooks/Defining-a-Heuristic-Policy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_greedy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    action = getaction(actions(mdp)[action])\n",
    "    timesteps, laststate = getstate(states(mdp)[i])[2]\n",
    "    push!(x, (p2.A^timesteps)[laststate,1])\n",
    "    push!(y, action[2])\n",
    "end\n",
    "scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    action = getaction(actions(mdp)[action])\n",
    "    timesteps, laststate = getstate(states(mdp)[i])[2]\n",
    "    push!(x, (p2.A^timesteps)[laststate,1])\n",
    "    push!(y, action[2])\n",
    "end\n",
    "scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Two Markov chains of two states each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use DiscreteNonParametric instead of 0-variance Normal distn.\n",
    "p1 = HMM([0.7 0.3; 0.3 0.7], [Normal(0.5, 0), Normal(2.0, 0)])\n",
    "p2 = HMM([0.9 0.1; 0.1 0.9], [Normal(1.0,0), Normal(3.0,0)])\n",
    "mdp = MonitoringMDP([100, 100], [p1, p2], [0.05, 0.15], 0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smdp = SparseTabularMDP(mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SparseValueIterationSolver(max_iterations=100, belres=1e-6, verbose=true)\n",
    "res = solve(solver, smdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = [], [], []\n",
    "for (i, action) in enumerate(res.policy)\n",
    "    state = getstate(states(mdp)[i])\n",
    "    timesteps, laststate = state[1]\n",
    "    push!(x, (p1.A^(timesteps+1))[laststate,1])\n",
    "    timesteps, laststate = state[2]\n",
    "    push!(y, (p2.A^(timesteps+1))[laststate,1])\n",
    "    push!(z, action)\n",
    "end\n",
    "# scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x, y, c=z)\n",
    "xlim(0,1)\n",
    "ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement https://juliapomdp.github.io/POMDPModelTools.jl/latest/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://juliapomdp.github.io/POMDPSimulators.jl/stable/parallel/#Parallel-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
